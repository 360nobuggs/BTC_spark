{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer, TopicPartition\n",
    "consumer = KafkaConsumer(\n",
    "    bootstrap_servers='10.204.131.11:9092',\n",
    "    group_id='leitor'\n",
    ")\n",
    "consumer.assign([TopicPartition('g03', 0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('bigdata')\\\n",
    "        .enableHiveSupport().getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from HDFS and evaluate result to consume in kafka\n",
    "def handleData(data):\n",
    "  print(\"WIP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in consumer:\n",
    "    #print (message)\n",
    "    value = message.value.decode('utf-8')\n",
    "    handleData(value)\n",
    "    #print(value)\n",
    "    if consumer.position(TopicPartition('g03', 0)) == consumer.end_offsets([TopicPartition('g03', 0)])[TopicPartition('g03', 0)]:\n",
    "        break\n",
    "\n",
    "consumer.commit()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
